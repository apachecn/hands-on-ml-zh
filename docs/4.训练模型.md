# 四、训练模型

在之前的描述中，我们通常把机器学习模型和训练算法当作黑箱子来处理。如果你实践过前几章的一些示例，你惊奇的发现你可以优化回归系统，改进数字图像的分类器，你甚至可以零基础搭建一个垃圾邮件的分类器，但是你却对它们内部的工作流程一无所知。事实上，许多场合你都不需要知道这些黑箱子的内部有什么，干了什么。

然而，如果你对其内部的工作流程有一定了解的话，当面对一个机器学习任务时候，这些理论可以帮助你快速的找到恰当的机器学习模型，合适的训练算法，以及一个好的假设集。同时，了解黑箱子内部的构成，有助于你更好地调试参数以及更有效的误差分析。本章讨论的大部分话题对于机器学习模型的理解，构建，以及神经网络（详细参考本书的第二部分）的训练都是非常重要的。

首先我们将以一个简单的线性回归模型为例，讨论两种不同的训练方法来得到模型的最优解：

+ 直接使用封闭方程进行求根运算，得到模型在当前训练集上的最优参数（即在训练集上使损失函数达到最小值的模型参数）

+ 使用迭代优化方法：梯度下降（GD），在训练集上，它可以逐渐调整模型参数以获得最小的损失函数，最终，参数会收敛到和第一种方法相同的的值。同时，我们也会介绍一些梯度下降的变体形式：批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD），在第二部分的神经网络部分，我们会多次使用它们。

接下来，我们将研究一个更复杂的模型：多项式回归，它可以拟合非线性数据集，由于它比线性模型拥有更多的参数，于是它更容易出现模型的过拟合。因此，我们将介绍如何通过学习曲线去判断模型是否出现了过拟合，并介绍几种正则化方法以减少模型出现过拟合的风险。

最后，我们将介绍两个常用于分类的模型：Logistic回归和Softmax回归

> 提示
> 
> 在本章中包含许多数学公式，以及一些线性代数和微积分基本概念。为了理解这些公式，你需要知道什么是向量，什么是矩阵，以及它们直接是如何转化的，以及什么是点积，什么是矩阵的逆，什么是偏导数。如果你对这些不是很熟悉的话，你可以阅读本书提供的 Jupyter 在线笔记，它包括了线性代数和微积分的入门指导。对于那些不喜欢数学的人，你也应该快速简单的浏览这些公式。希望它足以帮助你理解大多数的概念。

## 线性回归

在第一章，我们介绍了一个简单的生活满意度回归模型:

![life\_satisfaction = \theta_{0} + \theta_{1} * GDP\_per\_capita](../images/tex-2b4fc5fcdceb2e12c666415e9ebb793a.gif)

这个模型仅仅是输入量`GDP_per_capita`的线性函数，![\theta _{0}](../images/tex-3d7e996adfe310516ca31c796df1ce8c.gif) 和 ![\theta _{1}](../images/tex-92c9134527161fd7453fe848b821d8c7.gif) 是这个模型的参数，线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。如公式 4-1：

公式 4-1：线性回归预测模型

![\hat{y} = \theta _{0} + \theta _{1}x _{1}+\theta _{2}x _{2}+\dots+\theta _{n}x _{n}](../images/tex-f99876b625a13a0aad9631f61d934a61.gif)

+ ![\hat{y}](../images/tex-5d28a7ba1a44a73b8c2ed21321697c59.gif) 表示预测结果
+ ![n](../images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif) 表示特征的个数
+ ![x_{i}](../images/tex-05e42209d67fe1eb15a055e9d3b3770e.gif) 表示第`i`个特征的值
+ ![\theta_{j}](../images/tex-4c5645510c6b63f6bd0c770fceea4ac1.gif) 表示第`j`个参数（包括偏置项 ![\theta _{0}](../images/tex-3d7e996adfe310516ca31c796df1ce8c.gif) 和特征权重值 ![\theta _{1},\theta _{2},\dots,\theta _{n}](../images/tex-54552e28fe2d86eb74de3db6abb5aab8.gif)）

上述公式可以写成更为简洁的向量形式，如公式 4-2：

公式 4-2：线性回归预测模型（向量形式）

![\hat{y} = h _{\theta} (\mathbf{x})= \theta^T  \cdot \mathbf{x}](../images/tex-5da22015388cdeacf9c75c3511592953.gif)

+ ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif) 表示模型的参数向量包括偏置项 ![\theta _{0}](../images/tex-3d7e996adfe310516ca31c796df1ce8c.gif) 和特征权重值 ![\theta _{1}](../images/tex-92c9134527161fd7453fe848b821d8c7.gif) 到 ![\theta _{n}](../images/tex-f5c4858c5101df83fa23d1f57981c12b.gif)
+ ![\theta^T](../images/tex-616944242afb697770bd2354c57e4773.gif) 表示向量![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif)的转置（行向量变为了列向量）
+ ![\mathbf{x}](../images/tex-70e59a996bd69a0c21878b4093375e92.gif) 为每个样本中特征值的向量形式，包括 ![x _{1}](../images/tex-f1dd81ad21180e074e52e8cccdcbb172.gif) 到 ![x_{n}](../images/tex-67b68721103b5a16194f4b3e3ec222db.gif)，而且 ![x_0](../images/tex-3e0d691f3a530e6c7e079636f20c111b.gif) 恒为 1
+ ![\theta^T  \cdot \mathbf{x}](../images/tex-dc285d84f74bea2336104ee5eafff150.gif) 表示 ![\theta^T](../images/tex-616944242afb697770bd2354c57e4773.gif) 和![  \mathbf{x}](../images/tex-7186298c04fa42047e7992afaf52dfe0.gif) 的点积
+ ![h_{\theta}](../images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif) 表示参数为 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif) 的假设函数

怎么样去训练一个线性回归模型呢？好吧，回想一下，训练一个模型指的是设置模型的参数使得这个模型在训练集的表现较好。为此，我们首先需要找到一个衡量模型好坏的评定方法。在第二章，我们介绍到在回归模型上，最常见的评定标准是均方根误差（RMSE，详见公式 2-1）。因此，为了训练一个线性回归模型，你需要找到一个 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif) 值，它使得均方根误差（标准误差）达到最小值。实践过程中，最小化均方误差比最小化均方根误差更加的简单，这两个过程会得到相同的 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif)，因为函数在最小值时候的自变量，同样能使函数的方根运算得到最小值。

在训练集 ![\mathbf{X}](../images/tex-ca340abf4b48dc6d816137fbadf58b53.gif) 上使用公式 4-3 来计算线性回归假设 ![h_{\theta}](../images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif) 的均方差（![MSE](../images/tex-a9fc1a03386ae38b64e06c8172994963.gif)）。

公式 4-3：线性回归模型的 MSE 损失函数

![MSE (\mathbf{X},h_{\theta}) = \frac{1}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}^2](../images/tex-e42dee8953b9b2be4a3ed6f8c09e5314.gif)

公式中符号的含义大多数都在第二章（详见“符号”）进行了说明，不同的是：为了突出模型的参数向量 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif)，使用 ![h_{\theta}](../images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif) 来代替 ![h](../images/tex-2510c39011c5be704182423e3a695e91.gif)。以后的使用中为了公式的简洁，使用 ![MSE(\theta)](../images/tex-e6e3116680e8cef8739f29e51e9ae4dc.gif) 来代替 ![MSE(\mathbf{X},h _{\theta})](../images/tex-8fdec996997ab7fb44bf97399fda93c7.gif)。

### 正态方程

为了找到最小化损失函数的 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif) 值，可以采用公式解，换句话说，就是可以通过解正态方程直接得到最后的结果。

公式 4-4：正态方程

![\hat{\theta} = ({\mathbf{X}}^T\cdot\mathbf{X})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y}](../images/tex-43bfb04cdbbd85ad21489e8e2dc853ed.gif)

+ ![\hat{\theta}](../images/tex-0678caa04da34220a4e8dc041488b618.gif) 指最小化损失 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif) 的值
+ ![\mathbf{y}](../images/tex-971f2023c1f5f54b8bd389bb06fa6d86.gif) 是一个向量，其包含了 ![y^{(1)}](../images/tex-a2c0f1b552d410257a2cc027b24757a9.gif) 到 ![y^{(m)}](../images/tex-a4bd3c895fe9f194e30d5ce53dbb5fee.gif) 的值

让我们生成一些近似线性的数据（如图 4-1）来测试一下这个方程。

```python
import numpy as np 
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
```

![](../images/chapter_4/图4-1.PNG)

图 4-1：随机线性数据集

现在让我们使用正态方程来计算 ![\hat{\theta}](../images/tex-0678caa04da34220a4e8dc041488b618.gif)，我们将使用 Numpy 的线性代数模块（`np.linalg`）中的`inv()`函数来计算矩阵的逆，以及`dot()`方法来计算矩阵的乘法。

```python 
X_b = np.c_[np.ones((100, 1)), X] 
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```

我们生产数据的函数实际上是 ![y = 4 + 3x_0 + 高斯噪声](../images/tex-751d6173162c5bb7b6294ca57e03d5b1.gif)。让我们看一下最后的计算结果。

```python 
>>> theta_best
array([[4.21509616],[2.77011339]])
```

我们希望最后得到的参数为 ![\theta_0=4,\theta_1=3](../images/tex-e62a4db03666029d1dc53713c5632ac6.gif) 而不是 ![\theta_0=3.865,\theta_1=3.139](../images/tex-4443b7e9e72b482475432b88c0e4fdd8.gif) （译者注：我认为应该是 ![\theta_0=4.2150,\theta_1=2.7701](../images/tex-74747d7f311ae50d3c361e82606617d1.gif)）。这已经足够了，由于存在噪声，参数不可能达到到原始函数的值。

现在我们能够使用 ![\hat{\theta}](../images/tex-0678caa04da34220a4e8dc041488b618.gif) 来进行预测：

```python 
>>> X_new = np.array([[0],[2]])
>>> X_new_b = np.c_[np.ones((2, 1)), X_new]
>>> y_predict = X_new_b.dot(theta_best)
>>> y_predict
array([[4.21509616],[9.75532293]])
```

画出这个模型的图像，如图 4-2

```python 
plt.plot(X_new,y_predict,"r-")
plt.plot(X,y,"b.")
plt.axis([0,2,0,15])
plt.show()
```

![](../images/chapter_4/图4-2.PNG)

图4-2：线性回归预测

使用下面的 Scikit-Learn 代码可以达到相同的效果：

```python
>>> from sklearn.linear_model import LinearRegression
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X,y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([4.21509616]),array([2.77011339]))
>>> lin_reg.predict(X_new)
array([[4.21509616],[9.75532293]])
```

### 计算复杂度

正态方程需要计算矩阵 ![{\mathbf{X}}^T\cdot\mathbf{X}](../images/tex-e5949d4f83eb4e13761f2b76ab62386e.gif) 的逆，它是一个 ![n * n](../images/tex-6679fc33e499a90a99b97201f4d00ed5.gif) 的矩阵（![n](../images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif) 是特征的个数）。这样一个矩阵求逆的运算复杂度大约在 ![O(n^{2.4})](../images/tex-31ba2ac7dbc438cef695358d6e49deb3.gif) 到 ![O(n^3)](../images/tex-4a7d22b39e93fbbcbe107e7a19e8bd34.gif) 之间，具体值取决于计算方式。换句话说，如果你将你的特征个数翻倍的话，其计算时间大概会变为原来的 5.3（![2^{2.4}](../images/tex-61f8abcb13be8d0a51b2868de491d3a8.gif)）到 8（![2^3](../images/tex-5b9a77af89d04a685b4f649da485aed3.gif)）倍。

> 提示
> 
> 当特征的个数较大的时候（例如：特征数量为 100000），正态方程求解将会非常慢。

有利的一面是，这个方程在训练集上对于每一个实例来说是线性的，其复杂度为 ![O(m)](../images/tex-0e2ae329177722b1818828e92b441032.gif)，因此只要有能放得下它的内存空间，它就可以对大规模数据进行训练。同时，一旦你得到了线性回归模型（通过解正态方程或者其他的算法），进行预测是非常快的。因为模型中计算复杂度对于要进行预测的实例数量和特征个数都是线性的。 换句话说，当实例个数变为原来的两倍多的时候（或特征个数变为原来的两倍多），预测时间也仅仅是原来的两倍多。

接下来，我们将介绍另一种方法去训练模型。这种方法适合在特征个数非常多，训练实例非常多，内存无法满足要求的时候使用。

## 梯度下降

梯度下降是一种非常通用的优化算法，它能够很好地解决一系列问题。梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。

假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif)的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。

具体来说，开始时，需要选定一个随机的![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif)（这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值（如图：4-3）。

![](../images/chapter_4/图4-3.PNG)

图 4-3：梯度下降

在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的（如图 4-4）。

![](../images/chapter_4/图4-4.PNG)

图 4-4:学习率过小

另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案（如图 4-5）。

![](../images/chapter_4/图4-5.PNG)

图 4-5：学习率过大

最后，并不是所有的损失函数看起来都像一个规则的碗。它们可能是洞，山脊，高原和各种不规则的地形，使它们收敛到最小值非常的困难。 图 4-6 显示了梯度下降的两个主要挑战：如果随机初始值选在了图像的左侧，则它将收敛到局部最小值，这个值要比全局最小值要大。 如果它从右侧开始，那么跨越高原将需要很长时间，如果你早早地结束训练，你将永远到不了全局最小值。

![](../images/chapter_4/图4-6.PNG)

图 4-6：梯度下降的陷阱

幸运的是线性回归模型的均方差损失函数是一个凸函数，这意味着如果你选择曲线上的任意两点，它们的连线段不会与曲线发生交叉（译者注：该线段不会与曲线有第三个交点）。这意味着这个损失函数没有局部最小值，仅仅只有一个全局最小值。同时它也是一个斜率不能突变的连续函数。这两个因素导致了一个好的结果：梯度下降可以无限接近全局最小值。（只要你训练时间足够长，同时学习率不是太大 ）。

事实上，损失函数的图像呈现碗状，但是不同特征的取值范围相差较大的时，这个碗可能是细长的。图 4-7 展示了梯度下降在不同训练集上的表现。在左图中，特征 1 和特征 2 有着相同的数值尺度。在右图中，特征 1 比特征2的取值要小的多，由于特征 1 较小，因此损失函数改变时，![\theta_1](../images/tex-7672d625e9a2492987c50d3b87c04349.gif) 会有较大的变化，于是这个图像会在![\theta_1](../images/tex-7672d625e9a2492987c50d3b87c04349.gif)轴方向变得细长。

![](../images/chapter_4/图4-7.PNG)

图 4-7：有无特征缩放的梯度下降

正如你看到的，左面的梯度下降可以直接快速地到达最小值，然而在右面的梯度下降第一次前进的方向几乎和全局最小值的方向垂直，并且最后到达一个几乎平坦的山谷，在平坦的山谷走了很长时间。它最终会达到最小值，但它需要很长时间。

> 提示
> 
> 当我们使用梯度下降的时候，应该确保所有的特征有着相近的尺度范围（例如：使用 Scikit Learn 的 `StandardScaler`类），否则它将需要很长的时间才能够收敛。

这幅图也表明了一个事实：训练模型意味着找到一组模型参数，这组参数可以在训练集上使得损失函数最小。这是对于模型参数空间的搜索，模型的参数越多，参数空间的维度越多，找到合适的参数越困难。例如在300维的空间找到一枚针要比在三维空间里找到一枚针复杂的多。幸运的是线性回归模型的损失函数是凸函数，这个最优参数一定在碗的底部。

### 批量梯度下降

使用梯度下降的过程中，你需要计算每一个 ![\theta_j](../images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif) 下损失函数的梯度。换句话说，你需要计算当![\theta_j](../images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif)变化一点点时，损失函数改变了多少。这称为偏导数，它就像当你面对东方的时候问："我脚下的坡度是多少？"。然后面向北方的时候问同样的问题（如果你能想象一个超过三维的宇宙，可以对所有的方向都这样做）。公式 4-5 计算关于 ![\theta_j](../images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif) 的损失函数的偏导数，记为 ![\frac{\partial }{\partial \theta_j}MSE(\theta)](../images/tex-5c3b6b00cffc9732138715003b0e557a.gif)。

公式 4-5： 损失函数的偏导数

![\frac{\partial }{\partial \theta_j}MSE(\theta)=\frac{2}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}{x_j}^{(i)}](../images/tex-3a877201402d7cd2b9d3f5b726d22b24.gif)

为了避免单独计算每一个梯度，你也可以使用公式 4-6 来一起计算它们。梯度向量记为 ![\nabla_{\theta}MSE(\theta)](../images/tex-2046197f9491e759ad46d9ee09227c01.gif)，其包含了损失函数所有的偏导数（每个模型参数只出现一次）。

公式 4-6：损失函数的梯度向量

![\nabla_{\theta}MSE(\theta)= \left(\begin{matrix} \frac{\partial }{\partial \theta_0}MSE(\theta)\\ \frac{\partial }{\partial \theta_1}MSE(\theta)\\ \vdots \\ \frac{\partial }{\partial \theta_n}MSE(\theta)\\ \end{matrix}\right)=\frac{2}{m}{\mathbf{X}}^T\cdot{(\mathbf{X}\cdot\theta-y)}](../images/tex-a007d1162d9c4957e8336b4b10d5fda3.gif)

> 提示
> 
> 在这个方程中每一步计算时都包含了整个训练集 ![\mathbf{X}](../images/tex-ca340abf4b48dc6d816137fbadf58b53.gif)，这也是为什么这个算法称为批量梯度下降：每一次训练过程都使用所有的的训练数据。因此，在大数据集上，其会变得相当的慢（但是我们接下来将会介绍更快的梯度下降算法）。然而，梯度下降的运算规模和特征的数量成正比。训练一个数千数量特征的线性回归模型使用*梯度下降要比使用正态方程快的多。



一旦求得了方向是上山的梯度向量，你就可以向着相反的方向去下山。这意味着从 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif) 中减去 ![\nabla_{\theta}MSE(\theta)](../images/tex-2046197f9491e759ad46d9ee09227c01.gif)。学习率 ![\eta](../images/tex-ffe9f913124f345732e9f00fa258552e.gif) 和梯度向量的积决定了下山时每一步的大小，如公式 4-7。

公式 4-7：梯度下降步长

![\theta^{(next\ step)}=\theta - \eta\nabla_{\theta}MSE(\theta)](../images/tex-e25626090e2c767f539550e3c02fa6c8.gif)

让我们看一下这个算法的应用：

```python
eta = 0.1 # 学习率
n_iterations = 1000
m = 100

theta = np.random.randn(2,1) # 随机初始值

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
```

这不是太难，让我们看一下最后的结果 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif)：

```python
>>> theta
array([[4.21509616],[2.77011339]])
```

看！正态方程的表现非常好。完美地求出了梯度下降的参数。但是当你换一个学习率会发生什么？图 4-8 展示了使用了三个不同的学习率进行梯度下降的前 10 步运算（虚线代表起始位置）。

![](../images/chapter_4/图4-8.PNG)

图 4-8：不同学习率的梯度下降

在左面的那副图中，学习率是最小的，算法几乎不能求出最后的结果，而且还会花费大量的时间。在中间的这幅图中，学习率的表现看起来不错，仅仅几次迭代后，它就收敛到了最后的结果。在右面的那副图中，学习率太大了，算法是发散的，跳过了所有的训练样本，同时每一步都离正确的结果越来越远。

为了找到一个好的学习率，你可以使用网格搜索（详见第二章）。当然，你一般会限制迭代的次数，以便网格搜索可以消除模型需要很长时间才能收敛这一个问题。

你可能想知道如何选取迭代的次数。如果它太小了，当算法停止的时候，你依然没有找到最优解。如果它太大了，算法会非常的耗时同时后来的迭代参数也不会发生改变。一个简单的解决方法是：设置一个非常大的迭代次数，但是当梯度向量变得非常小的时候，结束迭代。非常小指的是：梯度向量小于一个值 ![\varepsilon](../images/tex-f8b1c5a729a09649c275fca88976d8dd.gif)（称为容差）。这时候可以认为梯度下降几乎已经达到了最小值。

> 收敛速率：
> 
> 当损失函数是凸函数，同时它的斜率不能突变（就像均方差损失函数那样），那么它的批量梯度下降算法固定学习率之后，它的收敛速率是 ![O(\frac{1}{iterations})](../images/tex-2381690b73b9410210542c6128e83b96.gif)。换句话说，如果你将容差 ![\varepsilon](../images/tex-f8b1c5a729a09649c275fca88976d8dd.gif) 缩小 10 倍后（这样可以得到一个更精确的结果），这个算法的迭代次数大约会变成原来的 10 倍。

### 随机梯度下降

批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。

另一方面，由于它的随机性，与批量梯度下降相比，其呈现出更多的不规律性：它到达最小值不是平缓的下降，损失函数会忽高忽低，只是在大体上呈下降趋势。随着时间的推移，它会非常的靠近最小值，但是它不会停止在一个值上，它会一直在这个值附近摆动（如图 4-9）。因此，当算法停止的时候，最后的参数还不错，但不是最优值。

![](../images/chapter_4/图4-9.PNG)

图4-9：随机梯度下降

当损失函数很不规则时（如图 4-6），随机梯度下降算法能够跳过局部最小值。因此，随机梯度下降在寻找全局最小值上比批量梯度下降表现要好。

虽然随机性可以很好的跳过局部最优值，但同时它却不能达到最小值。解决这个难题的一个办法是逐渐降低学习率。 开始时，走的每一步较大（这有助于快速前进同时跳过局部最小值），然后变得越来越小，从而使算法到达全局最小值。 这个过程被称为模拟退火，因为它类似于熔融金属慢慢冷却的冶金学退火过程。 决定每次迭代的学习率的函数称为`learning schedule`。 如果学习速度降低得过快，你可能会陷入局部最小值，甚至在到达最小值的半路就停止了。 如果学习速度降低得太慢，你可能在最小值的附近长时间摆动，同时如果过早停止训练，最终只会出现次优解。

下面的代码使用一个简单的`learning schedule`来实现随机梯度下降：

```python
n_epochs = 50 
t0, t1 = 5, 50  #learning_schedule的超参数

def learning_schedule(t):
    return t0 / (t + t1)

theta = np.random.randn(2,1)

for epoch in range(n_epochs):
    for i in range(m):
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta)-yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
```

按习惯来讲，我们进行 ![m](../images/tex-6f8f57715090da2632453988d9a1501b.gif) 轮的迭代，每一轮迭代被称为一代。在整个训练集上，随机梯度下降迭代了 1000 次时，一般在第 50 次的时候就可以达到一个比较好的结果。

```python
>>> theta
array([[4.21076011],[2.748560791]])
```

图 4-10 展示了前 10 次的训练过程（注意每一步的不规则程度）。

![](../images/chapter_4/图4-10.PNG)

图 4-10：随机梯度下降的前10次迭代

由于每个实例的选择是随机的，有的实例可能在每一代中都被选到，这样其他的实例也可能一直不被选到。如果你想保证每一代迭代过程，算法可以遍历所有实例，一种方法是将训练集打乱重排，然后选择一个实例，之后再继续打乱重排，以此类推一直进行下去。但是这样收敛速度会非常的慢。

通过使用 Scikit-Learn 完成线性回归的随机梯度下降，你需要使用`SGDRegressor`类，这个类默认优化的是均方差损失函数。下面的代码迭代了 50 代，其学习率 ![\eta](../images/tex-ffe9f913124f345732e9f00fa258552e.gif) 为0.1（`eta0=0.1`），使用默认的`learning schedule`（与前面的不一样），同时也没有添加任何正则项（`penalty = None`）：

```python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)
sgd_reg.fit(X,y.ravel())
```

你可以再一次发现，这个结果非常的接近正态方程的解：

```
>>> sgd_reg.intercept_, sgd_reg.coef_
(array([4.18380366]),array([2.74205299]))
```

### 小批量梯度下降

最后一个梯度下降算法，我们将介绍小批量梯度下降算法。一旦你理解了批量梯度下降和随机梯度下降，再去理解小批量梯度下降是非常简单的。在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。

小批量梯度下降在参数空间上的表现比随机梯度下降要好的多，尤其在有大量的小型实例集时。作为结果，小批量梯度下降会比随机梯度更靠近最小值。但是，另一方面，它有可能陷在局部最小值中（在遇到局部最小值问题的情况下，和我们之前看到的线性回归不一样）。 图4-11显示了训练期间三种梯度下降算法在参数空间中所采用的路径。 他们都接近最小值，但批量梯度的路径最后停在了最小值，而随机梯度和小批量梯度最后都在最小值附近摆动。 但是，不要忘记，批次梯度需要花费大量时间来完成每一步，但是，如果你使用了一个较好的`learning schedule`，随机梯度和小批量梯度也可以得到最小值。

![](../images/chapter_4/图4-11.PNG)

图 4-11：参数空间的梯度下降路径

让我比较一下目前我们已经探讨过的对线性回归的梯度下降算法。如表 4-1 所示，其中 ![m](../images/tex-6f8f57715090da2632453988d9a1501b.gif) 表示训练样本的个数，![n](../images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif) 表示特征的个数。

表 4-1：比较线性回归的不同梯度下降算法

![](../images/chapter_4/表4-1.PNG)

> 提示
> 
> 上述算法在完成训练后，得到的参数基本没什么不同，它们会得到非常相似的模型，最后会以一样的方式去进行预测。

## 多项式回归

如果你的数据实际上比简单的直线更复杂呢？ 令人惊讶的是，你依然可以使用线性模型来拟合非线性数据。 一个简单的方法是对每个特征进行加权后作为新的特征，然后训练一个线性模型在这个扩展的特征集。 这种方法称为多项式回归。

让我们看一个例子。 首先，我们根据一个简单的二次方程（并加上一些噪声，如图 4-12）来生成一些非线性数据：

```python
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)
```

![](../images/chapter_4/图4-12.PNG)

图 4-12：生产加入噪声的非线性数据

很清楚的看出，直线不能恰当的拟合这些数据。于是，我们使用 Scikit-Learning 的`PolynomialFeatures`类进行训练数据集的转换，让训练集中每个特征的平方（2 次多项式）作为新特征（在这种情况下，仅存在一个特征）：

```python
>>> from sklearn.preprocessing import PolynomialFeatures
>>> poly_features = PolynomialFeatures(degree=2,include_bias=False)
>>> X_poly = poly_features.fit_transform(X)
>>> X[0]
array([-0.75275929])
>>> X_poly[0]
array([-0.75275929, 0.56664654])
```

`X_poly`现在包含原始特征![X](../images/tex-02129bb861061d1a052c592e2dc6b383.gif)并加上了这个特征的平方 ![X^2](../images/tex-75299c2520ca389119694b3da7cc7a84.gif)。现在你可以在这个扩展训练集上使用`LinearRegression`模型进行拟合，如图 4-13：

```python
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X_poly, y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([ 1.78134581]), array([[ 0.93366893, 0.56456263]]))
```

![](../images/chapter_4/图4-13.PNG)

图 4-13：多项式回归模型预测

还是不错的，模型预测函数 ![\hat{y}=0.56x_1^2+0.93x_1+1.78](../images/tex-24ff843f62db9062b4bb1fba4040c10f.gif)，事实上原始函数为 ![y=0.5x_1^2+1.0x_1+2.0](../images/tex-6f58e3449c2d62d9634b79c0484c14ac.gif) 再加上一些高斯噪声。

请注意，当存在多个特征时，多项式回归能够找出特征之间的关系（这是普通线性回归模型无法做到的）。 这是因为`LinearRegression`会自动添加当前阶数下特征的所有组合。例如，如果有两个特征 ![a,b](../images/tex-b345e1dc09f20fdefdea469f09167892.gif)，使用 3 阶（`degree=3`）的`LinearRegression`时，不仅有 ![a^2,a^3,b^2](../images/tex-e2319eb828681ba30bf7e05e07d7f5fa.gif) 以及 ![b^3](../images/tex-2d275b176c3436e8981c70371f474c9c.gif)，同时也会有它们的其他组合项 ![ab,a^2b,ab^2](../images/tex-e373951eded9d8d1fddb4db810c5069f.gif)。

> 提示
> 
> `PolynomialFeatures(degree=d)`把一个包含 ![n](../images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif) 个特征的数组转换为一个包含 ![\frac{(n+d)!}{d!n!}](../images/tex-bbbca49c7d6afb0938a5c37bb5b5fbcf.gif) 特征的数组，![n!](../images/tex-388f554901ba5d77339eec8b26beebea.gif) 表示 ![n](../images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif) 的阶乘，等于 ![1 * 2 * 3 \cdots * n](../images/tex-62b4db05802cc87cf8ed00845ce751af.gif)。小心大量特征的组合爆炸！

## 学习曲线

如果你使用一个高阶的多项式回归，你可能发现它的拟合程度要比普通的线性回归要好的多。例如，图 4-14 使用一个 300 阶的多项式模型去拟合之前的数据集，并同简单线性回归、2 阶的多项式回归进行比较。注意 300 阶的多项式模型如何摆动以尽可能接近训练实例。

![](../images/chapter_4/图4-14.PNG)

图 4-14：高阶多项式回归

当然，这种高阶多项式回归模型在这个训练集上严重过拟合了，线性模型则欠拟合。在这个训练集上，二次模型有着较好的泛化能力。那是因为在生成数据时使用了二次模型，但是一般我们不知道这个数据生成函数是什么，那我们该如何决定我们模型的复杂度呢？你如何告诉我你的模型是过拟合还是欠拟合？

在第二章，你可以使用交叉验证来估计一个模型的泛化能力。如果一个模型在训练集上表现良好，通过交叉验证指标却得出其泛化能力很差，那么你的模型就是过拟合了。如果在这两方面都表现不好，那么它就是欠拟合了。这种方法可以告诉我们，你的模型是太复杂还是太简单了。

另一种方法是观察学习曲线：画出模型在训练集上的表现，同时画出以训练集规模为自变量的训练集函数。为了得到图像，需要在训练集的不同规模子集上进行多次训练。下面的代码定义了一个函数，用来画出给定训练集后的模型学习曲线：

```python
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

def plot_learning_curves(model, X, y):
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
    train_errors, val_errors = [], []
    for m in range(1, len(X_train)):
        model.fit(X_train[:m], y_train[:m])
        y_train_predict = model.predict(X_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))
        val_errors.append(mean_squared_error(y_val_predict, y_val))
    plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="train")
    plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="val")
```

我们一起看一下简单线性回归模型的学习曲线（图 4-15）：

```python
lin_reg = LinearRegression()
plot_learning_curves(lin_reg, X, y)
```

![](../images/chapter_4/图4-15.PNG)

图 4-15：学习曲线

这幅图值得我们深究。首先，我们观察训练集的表现：当训练集只有一两个样本的时候，模型能够非常好的拟合它们，这也是为什么曲线是从零开始的原因。但是当加入了一些新的样本的时候，训练集上的拟合程度变得难以接受，出现这种情况有两个原因，一是因为数据中含有噪声，另一个是数据根本不是线性的。因此随着数据规模的增大，误差也会一直增大，直到达到高原地带并趋于稳定，在之后，继续加入新的样本，模型的平均误差不会变得更好或者更差。我们继续来看模型在验证集上的表现，当以非常少的样本去训练时，模型不能恰当的泛化，也就是为什么验证误差一开始是非常大的。当训练样本变多的到时候，模型学习的东西变多，验证误差开始缓慢的下降。但是一条直线不可能很好的拟合这些数据，因此最后误差会到达在一个高原地带并趋于稳定，最后和训练集的曲线非常接近。

上面的曲线表现了一个典型的欠拟合模型，两条曲线都到达高原地带并趋于稳定，并且最后两条曲线非常接近，同时误差值非常大。

> 提示
> 
> 如果你的模型在训练集上是欠拟合的，添加更多的样本是没用的。你需要使用一个更复杂的模型或者找到更好的特征。

现在让我们看一个在相同数据上10阶多项式模型拟合的学习曲线（图 4-16）：

```python
from sklearn.pipeline import Pipeline

polynomial_regression = Pipeline((
    ("poly_features", PolynomialFeatures(degree=10, include_bias=False)),
    ("sgd_reg", LinearRegression()),
))

plot_learning_curves(polynomial_regression, X, y)
```

这幅图像和之前的有一点点像，但是其有两个非常重要的不同点：

+ 在训练集上，误差要比线性回归模型低的多。
+ 图中的两条曲线之间有间隔，这意味模型在训练集上的表现要比验证集上好的多，这也是模型过拟合的显著特点。当然，如果你使用了更大的训练数据，这两条曲线最后会非常的接近。

![](../images/chapter_4/图4-16.PNG)

图4-16：多项式模型的学习曲线

> 提示
> 
> 改善模型过拟合的一种方法是提供更多的训练数据，直到训练误差和验证误差相等。

> 偏差和方差的权衡
> 
> 在统计和机器学习领域有个重要的理论：一个模型的泛化误差由三个不同误差的和决定：
> 
> + 偏差：泛化误差的这部分误差是由于错误的假设决定的。例如实际是一个二次模型，你却假设了一个线性模型。一个高偏差的模型最容易出现欠拟合。
> + 方差：这部分误差是由于模型对训练数据的微小变化较为敏感，一个多自由度的模型更容易有高的方差（例如一个高阶多项式模型），因此会导致模型过拟合。
> + 不可约误差：这部分误差是由于数据本身的噪声决定的。降低这部分误差的唯一方法就是进行数据清洗（例如：修复数据源，修复坏的传感器，识别和剔除异常值）。


## 线性模型的正则化

正如我们在第一和第二章看到的那样，降低模型的过拟合的好方法是正则化这个模型（即限制它）：模型有越少的自由度，就越难以拟合数据。例如，正则化一个多项式模型，一个简单的方法就是减少多项式的阶数。

对于一个线性模型，正则化的典型实现就是约束模型中参数的权重。 接下来我们将介绍三种不同约束权重的方法：Ridge 回归，Lasso 回归和 Elastic Net。

### 岭（Ridge）回归

岭回归（也称为 Tikhonov 正则化）是线性回归的正则化版：在损失函数上直接加上一个正则项 ![\alpha\sum_{i=1}^n\theta_i^2](../images/tex-a21c05b05e3a61cef53414437bae86cf.gif)。这使得学习算法不仅能够拟合数据，而且能够使模型的参数权重尽量的小。注意到这个正则项只有在训练过程中才会被加到损失函数。当得到完成训练的模型后，我们应该使用没有正则化的测量方法去评价模型的表现。

> 提示
> 
> 一般情况下，训练过程使用的损失函数和测试过程使用的评价函数是不一样的。除了正则化，还有一个不同：训练时的损失函数应该在优化过程中易于求导，而在测试过程中，评价函数更应该接近最后的客观表现。一个好的例子：在分类训练中我们使用对数损失（马上我们会讨论它）作为损失函数，但是我们却使用精确率/召回率来作为它的评价函数。

超参数 ![\alpha](../images/tex-7b7f9dbfea05c83784f8b85149852f08.gif) 决定了你想正则化这个模型的强度。如果 ![\alpha=0](../images/tex-2aa447f3144cccc2865e6268c583f0f3.gif) 那此时的岭回归便变为了线性回归。如果 ![\alpha](../images/tex-7b7f9dbfea05c83784f8b85149852f08.gif) 非常的大，所有的权重最后都接近于零，最后结果将是一条穿过数据平均值的水平直线。公式 4-8 是岭回归的损失函数：

公式 4-8：岭回归损失函数

![J(\theta)=MSE(\theta)+\alpha\frac{1}{2}\sum\limits_{i=1}^n\theta_i^2](../images/tex-de03ddd330336d12e33df21217bdab9d.gif)

值得注意的是偏差 ![\theta_0](../images/tex-dbc9011a370bca098d4752346ba71d5c.gif) 是没有被正则化的（累加运算的开始是 ![i=1](../images/tex-9a041ce63f6c28100344427c6d71837b.gif) 而不是 ![i=0](../images/tex-8a8f1e8e0a73d8e44a17653f830f7947.gif)）。如我定义 ![\mathbf{w}](../images/tex-17227d892ae518eab12eb3f0e596f1a0.gif) 作为特征的权重向量（![\theta_1](../images/tex-7672d625e9a2492987c50d3b87c04349.gif) 到 ![\theta_n](../images/tex-a3026e320c132de94f7c8ebb952bda60.gif)），那么正则项可以简写成 ![\frac{1}{2}{({\parallel \mathbf{w}\parallel_2})}^2](../images/tex-e236013a1d5cfb056aa71c770d62e4ed.gif)，其中 ![\parallel \cdot \parallel_2 ](../images/tex-3fd16a45c4fce610740da450e9f5a283.gif) 表示权重向量的 ![\ell_2](../images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 范数。对于梯度下降来说仅仅在均方差梯度向量（公式 4-6）加上一项 ![\alpha\mathbf{w}](../images/tex-b0bb6cfa6e49912f2da3f807dd931480.gif)。

> 提示
> 
> 在使用岭回归前，对数据进行放缩（可以使用`StandardScaler`）是非常重要的，算法对于输入特征的数值尺度（scale）非常敏感。大多数的正则化模型都是这样的。

图 4-17 展示了在相同线性数据上使用不同 ![\alpha](../images/tex-7b7f9dbfea05c83784f8b85149852f08.gif) 值的岭回归模型最后的表现。左图中，使用简单的岭回归模型，最后得到了线性的预测。右图中的数据首先使用 10 阶的`PolynomialFearures`进行扩展，然后使用`StandardScaler`进行缩放，最后将岭模型应用在处理过后的特征上。这就是带有岭正则项的多项式回归。注意当![\alpha](../images/tex-7b7f9dbfea05c83784f8b85149852f08.gif)增大的时候，导致预测曲线变得扁平（即少了极端值，多了一般值），这样减少了模型的方差，却增加了模型的偏差。

对线性回归来说，对于岭回归，我们可以使用封闭方程去计算，也可以使用梯度下降去处理。它们的缺点和优点是一样的。公式 4-9 表示封闭方程的解（矩阵 ![\mathbf{A}](../images/tex-6c6404adc033dfed51422fdaf7fa0494.gif) 是一个除了左上角有一个 ![0](../images/tex-cfcd208495d565ef66e7dff9f98764da.gif) 的 ![n \times n](../images/tex-50f17e5c11d610b19c0471830dc4dda1.gif) 的单位矩，这个 ![0](../images/tex-cfcd208495d565ef66e7dff9f98764da.gif) 代表偏差项。译者注：偏差 ![\theta_0](../images/tex-dbc9011a370bca098d4752346ba71d5c.gif) 不被正则化的）。

![](../images/chapter_4/图4-17.PNG)

图 4-17：岭回归

公式 4-9：岭回归的封闭方程的解

![\hat{\theta} = ({\mathbf{X}}^T\cdot\mathbf{X}+\alpha\mathbf{A})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y}](../images/tex-2ef1c91a9cc8eeb7da8227d4016d702e.gif)

下面是如何使用 Scikit-Learn 来进行封闭方程的求解（使用 Cholesky 法进行矩阵分解对公式 4-9 进行变形）:

```python
>>> from sklearn.linear_model import Ridge
>>> ridge_reg = Ridge(alpha=1, solver="cholesky")
>>> ridge_reg.fit(X, y)
>>> ridge_reg.predict([[1.5]])
array([[ 1.55071465]]
```

使用随机梯度法进行求解：

```python
>>> sgd_reg = SGDRegressor(penalty="l2")
>>> sgd_reg.fit(X, y.ravel())
>>> sgd_reg.predict([[1.5]])
array([[ 1.13500145]])
```

`penalty`参数指的是正则项的惩罚类型。指定“l2”表明你要在损失函数上添加一项：权重向量 ![\ell_2](../images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 范数平方的一半，这就是简单的岭回归。

### Lasso 回归

Lasso 回归（也称 Least Absolute Shrinkage，或者 Selection Operator Regression）是另一种正则化版的线性回归：就像岭回归那样，它也在损失函数上添加了一个正则化项，但是它使用权重向量的 ![\ell_1](../images/tex-8524eb1789cf2093cfccc4c297138c7f.gif) 范数而不是权重向量 ![\ell_2](../images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 范数平方的一半。（如公式 4-10）

公式 4-10：Lasso 回归的损失函数

![J(\theta)=MSE(\theta)+\alpha\sum\limits_{i=1}^n\left|\theta_i \right|](../images/tex-a78e85b9c0eb6446f86c17d6d2190b74.gif)

图 4-18 展示了和图 4-17 相同的事情，仅仅是用 Lasso 模型代替了 Ridge 模型，同时调小了 ![\alpha​](../images/tex-fc3ffc226f182559eb73483ccbe7ee65.gif) 的值。

![](../images/chapter_4/图4-18.PNG)

图 4-18：Lasso回归

Lasso 回归的一个重要特征是它倾向于完全消除最不重要的特征的权重（即将它们设置为零）。例如，右图中的虚线所示（![\alpha=10^{-7}](../images/tex-af5e712113f834592672c1a7e4f1bee2.gif)），曲线看起来像一条二次曲线，而且几乎是线性的，这是因为所有的高阶多项特征都被设置为零。换句话说，Lasso回归自动的进行特征选择同时输出一个稀疏模型（即，具有很少的非零权重）。

你可以从图 4-19 知道为什么会出现这种情况：在左上角图中，后背景的等高线（椭圆）表示了没有正则化的均方差损失函数（![\alpha=0](../images/tex-2aa447f3144cccc2865e6268c583f0f3.gif)），白色的小圆圈表示在当前损失函数上批量梯度下降的路径。前背景的等高线（菱形）表示![\ell_1](../images/tex-8524eb1789cf2093cfccc4c297138c7f.gif)惩罚，黄色的三角形表示了仅在这个惩罚下批量梯度下降的路径（![\alpha\rightarrow\infty](../images/tex-7c44c4fd9ee64f79d37dc97e3ceb3c17.gif)）。注意路径第一次是如何到达 ![\theta_1=0](../images/tex-224ba64bbd16cef44085c714ff69b794.gif)，然后向下滚动直到它到达 ![\theta_2=0](../images/tex-421570724060381e95985593de9d77c9.gif)。在右上角图中，等高线表示的是相同损失函数再加上一个 ![\alpha=0.5](../images/tex-d7e489c9bd21eb7274ea7acd2b4f6b5b.gif) 的 ![\ell_1](../images/tex-8524eb1789cf2093cfccc4c297138c7f.gif) 惩罚。这幅图中，它的全局最小值在 ![\theta_2=0](../images/tex-421570724060381e95985593de9d77c9.gif) 这根轴上。批量梯度下降首先到达 ![\theta_2=0](../images/tex-421570724060381e95985593de9d77c9.gif)，然后向下滚动直到达到全局最小值。 两个底部图显示了相同的情况，只是使用了 ![\ell_2](../images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 惩罚。 规则化的最小值比非规范化的最小值更接近于 ![\theta=0](../images/tex-cbc3c4cd0071f0ac61b8ce488ff05234.gif)，但权重不能完全消除。

![](../images/chapter_4/图4-19.PNG)

图 4-19：Ridge 回归和 Lasso 回归对比

> 提示
> 
> 在 Lasso 损失函数中，批量梯度下降的路径趋向与在低谷有一个反弹。这是因为在 ![\theta_2=0](../images/tex-421570724060381e95985593de9d77c9.gif) 时斜率会有一个突变。为了最后真正收敛到全局最小值，你需要逐渐的降低学习率。

Lasso 损失函数在 ![\theta_i=0(i=1,2,\cdots,n) ](../images/tex-e4df33b5fe9a1d3789867afe58c9564a.gif) 处无法进行微分运算，但是梯度下降如果你使用子梯度向量 ![\mathbf{g}](../images/tex-d324bced968eda52e62e58cb90c82c2d.gif) 后它可以在任何 ![\theta_i=0](../images/tex-a64a4a0de329e98ac7b25d532cd74a4d.gif) 的情况下进行计算。公式 4-11 是在 Lasso 损失函数上进行梯度下降的子梯度向量公式。

公式 4-11：Lasso 回归子梯度向量  

![g(\theta,J)=\nabla_{\theta}MSE(\theta)+ \alpha{\left(\begin{matrix} sign(\theta_1)\\ sign(\theta_2)\\ \vdots \\ sign(\theta_n)\\ \end{matrix}\right)}\ where\ sign(\theta_i)= \begin{cases} -1, &\theta_i<0 \\ 0, &\theta_i=0 \\ +1,&\theta_i>0 \\ \end{cases}](../images/tex-93eea6b5c197bbc8d7be8b4c14e9f8f3.gif)


下面是一个使用 Scikit-Learn 的`Lasso`类的小例子。你也可以使用`SGDRegressor(penalty="l1")`来代替它。

```python
>>> from sklearn.linear_model import Lasso
>>> lasso_reg = Lasso(alpha=0.1)
>>> lasso_reg.fit(X, y)
>>> lasso_reg.predict([[1.5]])
array([ 1.53788174]
```

### 弹性网络（ElasticNet）

弹性网络介于 Ridge 回归和 Lasso 回归之间。它的正则项是 Ridge 回归和 Lasso 回归正则项的简单混合，同时你可以控制它们的混合率 ![r](../images/tex-4b43b0aee35624cd95b910189b3dc231.gif)，当 ![r=0](../images/tex-2c3db681686c1b080e21688bf57b256a.gif) 时，弹性网络就是 Ridge 回归，当 ![r=1](../images/tex-7c6270537cb2cf1c86fd46bbc6975dd3.gif) 时，其就是 Lasso 回归。具体表示如公式 4-12。

公式 4-12：弹性网络损失函数

![J(\theta)=MSE(\theta)+r\alpha\sum\limits_{i=1}^n\left|\theta_i \right|+\frac{1-r}{2}\alpha\sum\limits_{i=1}^n\theta_i^2](../images/tex-e4da079f692fe35778bbdf1fdf120d99.gif)

那么我们该如何选择线性回归，岭回归，Lasso 回归，弹性网络呢？一般来说有一点正则项的表现更好，因此通常你应该避免使用简单的线性回归。岭回归是一个很好的首选项，但是如果你的特征仅有少数是真正有用的，你应该选择 Lasso 和弹性网络。就像我们讨论的那样，它两能够将无用特征的权重降为零。一般来说，弹性网络的表现要比 Lasso 好，因为当特征数量比样本的数量大的时候，或者特征之间有很强的相关性时，Lasso 可能会表现的不规律。下面是一个使用 Scikit-Learn `ElasticNet`（`l1_ratio`指的就是混合率 ![r](../images/tex-4b43b0aee35624cd95b910189b3dc231.gif)）的简单样本：

```python
>>> from sklearn.linear_model import ElasticNet
>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
>>> elastic_net.fit(X, y)
>>> elastic_net.predict([[1.5]])
array([ 1.54333232])
```


### 早期停止法（Early Stopping）

对于迭代学习算法，有一种非常特殊的正则化方法，就像梯度下降在验证错误达到最小值时立即停止训练那样。我们称为早期停止法。图 4-20 表示使用批量梯度下降来训练一个非常复杂的模型（一个高阶多项式回归模型）。随着训练的进行，算法一直学习，它在训练集上的预测误差（RMSE）自然而然的下降。然而一段时间后，验证误差停止下降，并开始上升。这意味着模型在训练集上开始出现过拟合。一旦验证错误达到最小值，便提早停止训练。这种简单有效的正则化方法被 Geoffrey Hinton 称为“完美的免费午餐”

![](../images/chapter_4/图4-20.PNG)

图 4-20：早期停止法

> 提示
> 
> 随机梯度和小批量梯度下降不是平滑曲线，你可能很难知道它是否达到最小值。 一种解决方案是，只有在验证误差高于最小值一段时间后（你确信该模型不会变得更好了），才停止，之后将模型参数回滚到验证误差最小值。

下面是一个早期停止法的基础应用：

```python
from sklearn.base import clone
sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,learning_rate="constant", eta0=0.0005)

minimum_val_error = float("inf")
best_epoch = None
best_model = None
for epoch in range(1000):
    sgd_reg.fit(X_train_poly_scaled, y_train)
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    val_error = mean_squared_error(y_val_predict, y_val)
    if val_error < minimum_val_error:
        minimum_val_error = val_error
        best_epoch = epoch
        best_model = clone(sgd_reg)
```

注意：当`warm_start=True`时，调用`fit()`方法后，训练会从停下来的地方继续，而不是从头重新开始。

## 逻辑回归

正如我们在第1章中讨论的那样，一些回归算法也可以用于分类（反之亦然）。 Logistic 回归（也称为 Logit 回归）通常用于估计一个实例属于某个特定类别的概率（例如，这电子邮件是垃圾邮件的概率是多少？）。 如果估计的概率大于 50%，那么模型预测这个实例属于当前类（称为正类，标记为“1”），反之预测它不属于当前类（即它属于负类 ，标记为“0”）。 这样便成为了一个二元分类器。

### 概率估计

那么它是怎样工作的？ 就像线性回归模型一样，Logistic 回归模型计算输入特征的加权和（加上偏差项），但它不像线性回归模型那样直接输出结果，而是把结果输入`logistic()`函数进行二次加工后进行输出（详见公式 4-13）。

公式 4-13：逻辑回归模型的概率估计（向量形式）

![\hat{p}=h_\theta(\mathbf{x})=\sigma(\theta^T  \cdot \mathbf{x})](../images/tex-9d6a43d7b758ed2c9684fbbb81f9f1e8.gif)

Logistic 函数（也称为 logit），用 ![\sigma()](../images/tex-8a5c57afba70b57b7079f9a60efd5370.gif) 表示，其是一个 sigmoid 函数（图像呈 S 型），它的输出是一个介于 0 和 1 之间的数字。其定义如公式 4-14 和图 4-21 所示。

公式 4-14：逻辑函数

![\sigma(t)=\frac{1}{1+exp(-t)}](../images/tex-bec5b0923c70096e7336e2debb62ce82.gif)

![](../images/chapter_4/图4-21.PNG)

图4-21：逻辑函数

一旦 Logistic 回归模型估计得到了 ![\mathbf{x}](../images/tex-70e59a996bd69a0c21878b4093375e92.gif) 属于正类的概率 ![\hat{p}=h_\theta(\mathbf{x})](../images/tex-ef33861da9c3ac0329d8c6963e856c71.gif)，那它很容易得到预测结果 ![\hat{y}](../images/tex-5d28a7ba1a44a73b8c2ed21321697c59.gif)（见公式 4-15）。

公式 4-15：逻辑回归预测模型

![\hat{y}= \begin{cases} 0, &\hat{p}<0.5 \\ 1,&\hat{p}\geq0.5 \\ \end{cases}](../images/tex-7b5aaccbd0d9d1237157caedb4e63579.gif)

注意当 ![t<0](../images/tex-ff7fa304ecf57b573120bafee2b1cb58.gif) 时 ![\sigma(t)<0.5](../images/tex-1c5fbe6b94a96589df8cc11e20542e17.gif)，当 ![t\geq0](../images/tex-046a5ffa6a06b4da61d932c172876785.gif)时![\sigma(t)\geq0.5](../images/tex-e12f172d616cf52e645466237de2557f.gif)，因此当![\theta^T  \cdot \mathbf{x}](../images/tex-dc285d84f74bea2336104ee5eafff150.gif) 是正数的话，逻辑回归模型输出 1，如果它是负数的话，则输出 0。

### 训练和损失函数

好，现在你知道了 Logistic 回归模型如何估计概率并进行预测。 但是它是如何训练的？ 训练的目的是设置参数向量 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif)，使得正例（![y=1](../images/tex-6a267007228f9f654a0d28dec6932c31.gif)）概率增大，负例（![y=0](../images/tex-fab37d6c4a697fe660387d3ff8e889a4.gif)）的概率减小，其通过在单个训练实例 ![\mathbf{x}](../images/tex-70e59a996bd69a0c21878b4093375e92.gif) 的损失函数来实现（公式 4-16）。

公式 4-16：单个样本的损失函数

![c(\theta)= \begin{cases} -log(\hat{p}), &y=1 \\ -log(1-\hat{p}),&y=0 \\ \end{cases}](../images/tex-81c37589241489382297c799f1fc6b45.gif)


这个损失函数是合理的，因为当 ![t](../images/tex-e358efa489f58062f10dd7316b65649e.gif) 接近 0 时，![-log(t)](../images/tex-0b092886509724e44645878e1391a65d.gif) 变得非常大，所以如果模型估计一个正例概率接近于 0，那么损失函数将会很大，同时如果模型估计一个负例的概率接近 1，那么损失函数同样会很大。 另一方面，当 ![t](../images/tex-e358efa489f58062f10dd7316b65649e.gif) 接近于 1 时，![-log(t)](../images/tex-0b092886509724e44645878e1391a65d.gif) 接近 0，所以如果模型估计一个正例概率接近于 0，那么损失函数接近于 0，同时如果模型估计一个负例的概率接近 0，那么损失函数同样会接近于 0， 这正是我们想的。

整个训练集的损失函数只是所有训练实例的平均值。可以用一个表达式（你可以很容易证明）来统一表示，称为对数损失，如公式 4-17 所示。

公式 4-17：逻辑回归的损失函数（对数损失）

![J(\theta)=-\frac{1}{m}\sum\limits_{i=1}^m\left[y^{(i)}log\left(\hat{p}^{(i)}\right)+\left(1-y^{(i)}\right)log\left(1-\hat{p}^{(i)}\right)\right]](../images/tex-b6a0de3f265cdeedc2ac1d0687fef2ea.gif)

但是这个损失函数对于求解最小化损失函数的 ![\theta](../images/tex-2554a2bb846cffd697389e5dc8912759.gif) 是没有公式解的（没有等价的正态方程）。 但好消息是，这个损失函数是凸的，所以梯度下降（或任何其他优化算法）一定能够找到全局最小值（如果学习速率不是太大，并且你等待足够长的时间）。公式 4-18 给出了损失函数关于第 ![j](../images/tex-363b122c528f54df4a0446b6bab05515.gif) 个模型参数 ![\theta_j](../images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif) 的偏导数。

公式 4-18：逻辑回归损失函数的偏导数

![\frac{\partial}{\partial \theta_j}J(\theta_j)=\frac{1}{m} \sum\limits_{i=1}^m{\left(\sigma\left(\theta^T \cdot \mathbf{x}^{(i)}\right)-y^{(i)}\right)}{x_j}^{(i)}](../images/tex-4e016baa468f852047bbbc1b171743ac.gif)

这个公式看起来非常像公式 4-5：首先计算每个样本的预测误差，然后误差项乘以第 ![j](../images/tex-363b122c528f54df4a0446b6bab05515.gif) 项特征值，最后求出所有训练样本的平均值。 一旦你有了包含所有的偏导数的梯度向量，你便可以在梯度向量上使用批量梯度下降算法。 也就是说：你已经知道如何训练 Logistic 回归模型。 对于随机梯度下降，你当然只需要每一次使用一个实例，对于小批量梯度下降，你将每一次使用一个小型实例集。

### 决策边界

我们使用鸢尾花数据集来分析 Logistic 回归。 这是一个著名的数据集，其中包含 150 朵三种不同的鸢尾花的萼片和花瓣的长度和宽度。这三种鸢尾花为：Setosa，Versicolor，Virginica（如图 4-22）。

![](../images/chapter_4/图4-22.PNG)

图4-22：三种不同的鸢尾花

让我们尝试建立一个分类器，仅仅使用花瓣的宽度特征来识别 Virginica，首先让我们加载数据：

```python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> list(iris.keys())
['data', 'target_names', 'feature_names', 'target', 'DESCR']
>>> X = iris["data"][:, 3:] # petal width
>>> y = (iris["target"] == 2).astype(np.int)
```

接下来，我们训练一个逻辑回归模型：

```python
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X, y)
```

我们来看看模型估计的花瓣宽度从 0 到 3 厘米的概率估计（如图 4-23）：

```python
X_new = np.linspace(0, 3, 1000).reshape(-1, 1)
y_proba = log_reg.predict_proba(X_new)
plt.plot(X_new, y_proba[:, 1], "g-", label="Iris-Virginica")
plt.plot(X_new, y_proba[:, 0], "b--", label="Not Iris-Virginica")
```

![](../images/chapter_4/图4-23.PNG)

图 4-23：概率估计和决策边界

Virginica 花的花瓣宽度（用三角形表示）在 1.4 厘米到 2.5 厘米之间，而其他种类的花（由正方形表示）通常具有较小的花瓣宽度，范围从 0.1 厘米到 1.8 厘米。注意，它们之间会有一些重叠。在大约 2 厘米以上时，分类器非常肯定这朵花是Virginica花（分类器此时输出一个非常高的概率值），而在1厘米以下时，它非常肯定这朵花不是 Virginica 花（不是 Virginica 花有非常高的概率）。在这两个极端之间，分类器是不确定的。但是，如果你使用它进行预测（使用`predict()`方法而不是`predict_proba()`方法），它将返回一个最可能的结果。因此，在 1.6 厘米左右存在一个决策边界，这时两类情况出现的概率都等于 50%：如果花瓣宽度大于 1.6 厘米，则分类器将预测该花是 Virginica，否则预测它不是（即使它有可能错了）：

```python
>>> log_reg.predict([[1.7], [1.5]])
array([1, 0])
```

图 4-24 表示相同的数据集，但是这次使用了两个特征进行判断：花瓣的宽度和长度。 一旦训练完毕，Logistic 回归分类器就可以根据这两个特征来估计一朵花是 Virginica 的可能性。 虚线表示这时两类情况出现的概率都等于 50%：这是模型的决策边界。 请注意，它是一个线性边界。每条平行线都代表一个分类标准下的两两个不同类的概率，从 15%（左下角）到 90%（右上角）。越过右上角分界线的点都有超过 90% 的概率是 Virginica 花。 

![](../images/chapter_4/图4-24.PNG)

图 4-24：线性决策边界

就像其他线性模型，逻辑回归模型也可以 ![\ell_1](../images/tex-8524eb1789cf2093cfccc4c297138c7f.gif) 或者 ![\ell_2](../images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 惩罚使用进行正则化。Scikit-Learn 默认添加了 ![\ell_2](../images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 惩罚。

> 注意
> 
> 在 Scikit-Learn 的`LogisticRegression`模型中控制正则化强度的超参数不是 ![\alpha](../images/tex-7b7f9dbfea05c83784f8b85149852f08.gif)（与其他线性模型一样），而是它的逆：![C](../images/tex-0d61f8370cad1d412f80b84d143e1257.gif)。 ![C](../images/tex-0d61f8370cad1d412f80b84d143e1257.gif) 的值越大，模型正则化强度越低。

### Softmax 回归

Logistic 回归模型可以直接推广到支持多类别分类，不必组合和训练多个二分类器（如第 3 章所述）， 其称为 Softmax 回归或多类别 Logistic 回归。 

这个想法很简单：当给定一个实例 ![\mathbf{x}](../images/tex-70e59a996bd69a0c21878b4093375e92.gif) 时，Softmax 回归模型首先计算 ![k](../images/tex-8ce4b16b22b58894aa86c421e8759df3.gif) 类的分数 ![s_k(\mathbf{x})](../images/tex-a840076f36cc4bef1947d97a65426bb3.gif)，然后将分数应用在`Softmax `函数（也称为归一化指数）上，估计出每类的概率。 计算 ![s_k(\mathbf{x})](../images/tex-a840076f36cc4bef1947d97a65426bb3.gif) 的公式看起来很熟悉，因为它就像线性回归预测的公式一样（见公式 4-19）。

公式 4-19：`k`类的 Softmax 得分

![s_k(\mathbf{x})= \theta^T  \cdot \mathbf{x}](../images/tex-ae05c183b5c444d17b885e8f7392e33c.gif)

注意，每个类都有自己独一无二的参数向量 ![\theta_k](../images/tex-9f888eddb683fe5f80f87f44bd727b08.gif)。 所有这些向量通常作为行放在参数矩阵 ![\Theta](../images/tex-b9dce96eb3d5a71b28f9f198c28d2d1b.gif) 中。

一旦你计算了样本 ![\mathbf{x}](../images/tex-70e59a996bd69a0c21878b4093375e92.gif) 的每一类的得分，你便可以通过`Softmax`函数（公式 4-20）估计出样本属于第 ![k](../images/tex-8ce4b16b22b58894aa86c421e8759df3.gif) 类的概率 ![\hat{p}_k](../images/tex-6d3e49353ea5640912d4b8ae768bdb32.gif)：通过计算 ![e](../images/tex-e1671797c52e15f763380b45e841ec32.gif) 的 ![s_k(\mathbf{x})](../images/tex-a840076f36cc4bef1947d97a65426bb3.gif) 次方，然后对它们进行归一化（除以所有分子的总和）。

公式 4-20：Softmax 函数

![\hat{p_k}=\sigma{(\mathbf{s}(\mathbf{x}))}k= \frac{exp\left(s_k(\mathbf{x})\right)} {\sum_{j=1}^{K}exp\left(s_j(\mathbf{x})\right)}](../images/tex-9fff59e9f5c122d355124bb1bf98c0ff.gif)

+ ![K](../images/tex-a5f3c6a11b03839d46af9fb43c97c188.gif) 表示有多少类
+ ![\mathbf{s}(\mathbf{x})](../images/tex-875757e99c779a8aaac585041fe96bb7.gif) 表示包含样本 ![\mathbf{x}](../images/tex-70e59a996bd69a0c21878b4093375e92.gif) 每一类得分的向量
+ ![\sigma{(\mathbf{s}(\mathbf{x}))_k}](../images/tex-7ccc7a43b364826d3d3caf874433ba74.gif) 表示给定每一类分数之后，实例 ![\mathbf{x}](../images/tex-70e59a996bd69a0c21878b4093375e92.gif) 属于第 ![k](../images/tex-8ce4b16b22b58894aa86c421e8759df3.gif) 类的概率


和 Logistic 回归分类器一样，Softmax 回归分类器将估计概率最高（它只是得分最高的类）的那类作为预测结果，如公式 4-21 所示。

公式 4-21：Softmax 回归模型分类器预测结果

![\hat{y}=argmax\ \sigma{(\mathbf{s}(\mathbf{x}))_k}=argmax \ s_k(\mathbf{x})=argmax \ \left( \theta_k^T  \cdot \mathbf{x}\right)](../images/tex-a294eed0207348e7e8d2b0ca72aabf83.gif)

+ `argmax`运算返回一个函数取到最大值的变量值。 在这个等式，它返回使 ![\sigma{(\mathbf{s}(\mathbf{x}))_k}](../images/tex-7ccc7a43b364826d3d3caf874433ba74.gif) 最大时的 ![k](../images/tex-8ce4b16b22b58894aa86c421e8759df3.gif) 的值

> 注意
> 
> Softmax 回归分类器一次只能预测一个类（即它是多类的，但不是多输出的），因此它只能用于判断互斥的类别，如不同类型的植物。 你不能用它来识别一张照片中的多个人。

现在我们知道这个模型如何估计概率并进行预测，接下来将介绍如何训练。我们的目标是建立一个模型在目标类别上有着较高的概率（因此其他类别的概率较低），最小化公式 4-22 可以达到这个目标，其表示了当前模型的损失函数，称为交叉熵，当模型对目标类得出了一个较低的概率，其会惩罚这个模型。 交叉熵通常用于衡量待测类别与目标类别的匹配程度（我们将在后面的章节中多次使用它）

公式 4-22：交叉熵

![J(\Theta)=-\frac{1}{m}\sum\limits_{i=1}^m\sum\limits_{k=1}^Ky_k^{(i)}log\left(\hat{p}_k^{(i)}\right)](../images/tex-b20e626988e6c696012b02def76d5c6a.gif)

+ 如果对于第 ![i](../images/tex-865c0c0b4ab0e063e5caa3387c1a8741.gif) 个实例的目标类是 ![k](../images/tex-8ce4b16b22b58894aa86c421e8759df3.gif)，那么 ![y_k^{(i)}=1](../images/tex-8691f6b1f536233c7a2929e62280ffa0.gif)，反之 ![y_k^{(i)}=0](../images/tex-c8dda7e9c58592db5a22add487f77cf9.gif)。

可以看出，当只有两个类（![K=2](../images/tex-5869b95a3404e737433d626520200848.gif)）时，此损失函数等同于 Logistic 回归的损失函数（对数损失；请参阅公式 4-17）。

> 交叉熵
> 
> 交叉熵源于信息论。假设你想要高效地传输每天的天气信息。如果有八个选项（晴天，雨天等），则可以使用3位对每个选项进行编码，因为 ![2^3=8](../images/tex-5f344a952e29992de54b8cfe645b2d5b.gif)。但是，如果你认为几乎每天都是晴天，更高效的编码“晴天”的方式是：只用一位（0）。剩下的七项使用四位（从 1 开始）。交叉熵度量每个选项实际发送的平均比特数。 如果你对天气的假设是完美的，交叉熵就等于天气本身的熵（即其内部的不确定性）。 但是，如果你的假设是错误的（例如，如果经常下雨）交叉熵将会更大，称为 Kullback-Leibler 散度（KL 散度）。
> 
> 两个概率分布 ![p](../images/tex-83878c91171338902e0fe0fb97a8c47a.gif) 和 ![q](../images/tex-7694f4a66316e53c8cdd9d9954bd611d.gif) 之间的交叉熵定义为：![H(p,q)=-\sum_xp(x)\log q(x)](../images/tex-6bc68f603b52e51645b4bbd318f8cdfe.gif)（分布至少是离散的）

这个损失函数关于 ![\theta_k](../images/tex-9f888eddb683fe5f80f87f44bd727b08.gif) 的梯度向量为公式 4-23：


公式 4-23：`k`类交叉熵的梯度向量

![\nabla_{\theta_k}J(\Theta)=\frac{1}{m}\sum\limits_{i=1}^m\left(\hat{p}_k^{(i)}-y_k^{(i)}\right)\mathbf{x}^{(i)}](../images/tex-5dfdb421c5936031346fc0e53a028caf.gif)

现在你可以计算每一类的梯度向量，然后使用梯度下降（或者其他的优化算法）找到使得损失函数达到最小值的参数矩阵 ![\Theta](../images/tex-b9dce96eb3d5a71b28f9f198c28d2d1b.gif)。

让我们使用 Softmax 回归对三种鸢尾花进行分类。当你使用`LogisticRregression`对模型进行训练时，Scikit Learn 默认使用的是一对多模型，但是你可以设置`multi_class`参数为“multinomial”来把它改变为 Softmax 回归。你还必须指定一个支持 Softmax 回归的求解器，例如“lbfgs”求解器（有关更多详细信息，请参阅 Scikit-Learn 的文档）。其默认使用 ![\ell_12](../images/tex-1599eafa9c8919b48a73745c7b9a5fc5.gif) 正则化，你可以使用超参数 ![C](../images/tex-0d61f8370cad1d412f80b84d143e1257.gif) 控制它。

```python
X = iris["data"][:, (2, 3)] # petal length, petal width
y = iris["target"]

softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10)
softmax_reg.fit(X, y)
```

所以下次你发现一个花瓣长为 5 厘米，宽为 2 厘米的鸢尾花时，你可以问你的模型你它是哪一类鸢尾花，它会回答 94.2% 是 Virginica 花（第二类），或者 5.8% 是其他鸢尾花。

```python
>>> softmax_reg.predict([[5, 2]])
array([2])
>>> softmax_reg.predict_proba([[5, 2]])
array([[ 6.33134078e-07, 5.75276067e-02, 9.42471760e-01]])是
```

![](../images/chapter_4/图4-25.PNG)

图 4-25：Softmax 回归的决策边界

图 4-25 用不同背景色表示了结果的决策边界。注意，任何两个类之间的决策边界是线性的。 该图的曲线表示 Versicolor 类的概率（例如，用 0.450 标记的曲线表示 45% 的概率边界）。注意模型也可以预测一个概率低于 50% 的类。 例如，在所有决策边界相遇的地方，所有类的估计概率相等，分别为 33%。

## 练习

1. 如果你有一个数百万特征的训练集，你应该选择哪种线性回归训练算法？
2. 假设你训练集中特征的数值尺度（scale）有着非常大的差异，哪种算法会受到影响？有多大的影响？对于这些影响你可以做什么？
3. 训练 Logistic 回归模型时，梯度下降是否会陷入局部最低点？
4. 在有足够的训练时间下，是否所有的梯度下降都会得到相同的模型参数？
5. 假设你使用批量梯度下降法，画出每一代的验证误差。当你发现验证误差一直增大，接下来会发生什么？你怎么解决这个问题？
6. 当验证误差升高时，立即停止小批量梯度下降是否是一个好主意？
7. 哪个梯度下降算法（在我们讨论的那些算法中）可以最快到达解的附近？哪个的确实会收敛？怎么使其他算法也收敛？
8. 假设你使用多项式回归，画出学习曲线，在图上发现学习误差和验证误差之间有着很大的间隙。这表示发生了什么？有哪三种方法可以解决这个问题？
9. 假设你使用岭回归，并发现训练误差和验证误差都很高，并且几乎相等。你的模型表现是高偏差还是高方差？这时你应该增大正则化参数 ![\alpha](../images/tex-7b7f9dbfea05c83784f8b85149852f08.gif)，还是降低它？
10. 你为什么要这样做：
  + 使用岭回归代替线性回归？
  + Lasso 回归代替岭回归？
  + 弹性网络代替 Lasso 回归？
11. 假设你想判断一副图片是室内还是室外，白天还是晚上。你应该选择二个逻辑回归分类器，还是一个 Softmax 分类器？
12. 在 Softmax 回归上应用批量梯度下降的早期停止法（不使用 Scikit-Learn）。


附录 A 提供了这些练习的答案。
